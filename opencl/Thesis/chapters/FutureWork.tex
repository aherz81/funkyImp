% !TEX root = ../main.tex

\chapter{Further Work}
\label{chap:future}
There are some issues and topics that did not fit within the scope of this thesis. For one, there is further work on the funkyIMP compiler. There are language features that have not been implemented in the OpenCL backend yet. Implementing these would make the compiler more useful in real world applications. These language features are discussed in section \ref{sect:future_compiler}. Then there is the model itself. While it predicts the run time sufficiently precisely in the majority of cases, there are some cases where it fails to predict the runtime accurately. Details of these issues can be found in section \ref{sect:future_model}. Additionally, in order to be viable for real world use, the model needs to be tested and adapted for other GPU architectures. This is discussed in section \ref{sect:future_othergpus}.


\section{Refining the model}
\label{sect:future_model}
While the model gives good, or at least acceptable\footnote{The prediction does not deviate by more than 50\%}, predictions for a large majority of the domain iterations, there are some cases where the model fails to predict the runtime with an acceptable precision. Consider, for example, the kernel given in figure \ref{fig:future_model_bad_kernel}. The results, given in figure \ref{fig:future_model_bad_result}, reveal a significant discrepancy between the prediction and the run time of the kernel. The predictions made utilizing the model give an estimation that is too high, on average, by a factor of about three. It is possible to output a detailed report in the form of a table, with predictions for the time spent on each operation. For the this kernel, the table shown in figure \ref{fig:future_model_bad} has been generated. \\

Two problems may be inferred from the data given in the table. The first problem is an incorrectly classified memory access. The analyzer is unable to recognize that the memory accesses \verb!matrix[x,y]! and \verb!matrix[x%HEIGHT, y%WIDTH]! always access the same elements. If they were recognized as identical, then the second one would be classified as cached, and therefore be assigned no costs\footnote{see section \ref{sect:model_access_classes}}. This is not the case here, as evidenced by the two \code{GLOBAL READ}s in figure \ref{fig:future_model_bad}. Memory accesses that are recognized as identical only show up as one access in the detailed report. The incorrectly assigned memory access is therefore treated as independent, and assigned the full run time cost. The second problem is hinted at by the costs assigned to the floating point division. The predicted time  that would be spent on that operation alone exceeds the observed run time for the kernel. This effect could be explained by shadowing, i.e. it is possible that the GPU performs the division while waiting for a memory access to complete, thus effectively shadowing the costs of the division. \\

Another problem is compiler optimizations. The OpenCL compiler runs optimizations over the generated kernel. There are several optimizations, such as available expressions or constant propagation, that simplify the expression without changing the result. Thus, the code that is generated by the compiler may be different from the code given to the module analyzing the domain iteration. These optimizations are specific to the compiler used on the particular OpenCL platform. Factoring in these optimizations would require examining the used compilers on a case by case basis, as the optimizations might vary according to the different platform vendors.

\begin{figure}
	\lstset{language=java}
	\lstset{morekeywords={cancel}}
	\lstset{morekeywords={global}}
	\lstset{morekeywords={constant}}
	\begin{lstlisting}
cancel new float[two_d{HEIGHT, WIDTH}].\[x,y]{
  matrix[x,y] * (y * (((0.585229f 
  	+ ((x - matrix[x % HEIGHT, y % WIDTH]) 
    - matrix[492 % HEIGHT, 958 % WIDTH])) 
    * (matrix[x,y] / 767.6984f)) 
    * 661.48236f))
};
	\end{lstlisting}
	\caption{This is a domain iteration for which the prediction is remarkably inaccurate. The identifiers \code{HEIGHT} and \code{WIDTH} are replaced by the actual sizes of the benchmark.}
	\label{fig:future_model_bad_kernel}
\end{figure}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			\begin{loglogaxis}[	xlabel=Number of Elements, 
							ylabel=Time in Âµs,
							legend entries={Runtime Prediction, Actual Runtime},
							legend style={at={(1.03, 0.5)}, anchor=west}]
				\addplot [red, error bars/.cd, y dir=both, y explicit] table [x=NumElements, y=RuntimePrediction]{data/badTestResult.csv};
				\addplot [blue, error bars/.cd, y dir=both, y explicit] table [x=NumElements, y=ActualRuntime, y error=StandardDeviation]{data/badTestResult.csv};
			\end{loglogaxis}
		\end{tikzpicture}
		\caption{This graph shows the runtime of the domain iteration shown in figure \ref{fig:future_model_bad_kernel}. It can be seen that there is a significant difference between the predicted and the actual runtime. To increase readability, both axes are logarithmic scale.}
		\label{fig:future_model_bad_result}
	\end{center}
\end{figure}
\newpage

\begin{figure}
	\begin{center}
		\begin{tabular}{r|c|l}
			\textbf{Cost Type} & \textbf{\# in Kernel} & \textbf{Time} \\
			\hline
			FLOAT ADD & 1 &	219.311 \\
			FLOAT SUB & 2 & 0.0 \\
			FLOAT MUL & 4 & 0.0 \\
			FLOAT DIV & 1 & 39362.439 \\
			INT ADD & 8 & 220.515 \\
			INT SUB & 0 & 0.0 \\
			INT MUL & 9 & 324.170 \\
			INT DIV & 4 & 6028.192 \\
			LOCAL ACCESS & 0 & 0.0 \\
			PRIVATE ACCESS & 2 & 0.0 \\
			GLOBAL WRITE & 0 & 0.0 \\
			CONSTANT GLOBAL READ & 1 & 3083.679 \\
			CACHED GLOBAL READ & 0 & 0.0 \\
			GLOBAL READ & 2 & 47479.643 \\
			COMPLEX GLOBAL READ & 0 & 0.0 \\
			BASE COST & 1 & 12748.737\\
			\hline
			
		\end{tabular}
	\end{center}
	\caption{This is the result of the analysis for the kernel shown in figure \ref{fig:future_model_bad_kernel}. and a matrix size of $8192\times 8192$. Although the access \code{matrix[x,y]} occurs twice, it is only registered once, as the second occurrence will be cached.}
	\label{fig:future_model_bad}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other GPU architectures}
\label{sect:future_othergpus}

The model presented in chapter \ref{chap:model} was developed on a NVidia GT-650M, a notebook GPU with the Kepler microarchitecture. In order to use the model for other GPUs, the benchmark suite had to be executed on these GPUs as well. Up to now, the other GPUs that the benchmark suite has been executed on are the NVidia Quadro K4000, a professional workstation GPU, the AMD Radeon 5770 desktop GPU and the Intel HD 4000 integrated GPU. \\

However, the evaluation has only been carried out on the GT-650M. This is due to the fact that the performance prediction is currently integrated into the funkyIMP compiler. Unfortunately, installing that compiler is a non-trivial process, and an installation on Microsoft Windows is not possible at all. The best course of action would be to factor the performance evaluation into an external tool that analyzes the generated OpenCL code directly. This would also increase the applicability of the analyzer, as it would no longer depend on the funkyIMP language constructs, and could instead support all OpenCL constructs. \\  

The Quadro GPU measurements look similar to the measurements taken from the GT-650M. This should not be not too surprising, as both GPUs share a common architecture, i.e. the Kepler microarchitecture. That being said, the Quadro K4000 is significantly faster than the GT-650M. The only notable behavioral difference is on division, which is as fast as the other basic operations. A diagram illustrating this can be found as figure \ref{fig:appendix_k4000_ops} in the appendix.\\


\subsection{Intel HD 4000 (Integrated in the Ivy Bridge CPU)}
\label{sect:future_othergpus_hd4000}
The Intel HD 4000 GPU is an GPU that is packaged on the Ivy Bridge Architecture Intel CPUs. Integrated graphics solutions are built differently than dedicated GPUs. For one, they typically do not have any dedicated video memory. Instead, they share them main memory of the system with the CPU. Some integrated solutions have access to caches, but they typically share some of these with the CPU. The HD 4000 is no exception in this respect. Nevertheless, the performance of the HD 4000 is comparable to, or even exceeds, the performance of entry-level dedicated GPUs available on the market at the time the Ivy Bridge CPUs were released. \cite{xbit2012review} \\

The Intel HD 4000 has a total number of 16 execution units, each clocked at 1.2GHz. It may use up to 1GB of the main memory of the system, and the processor's L3 cache. Additionally, it has its own set of faster caches. The maximum work-group size is 512. \cite{check2012hd4000}\\

The execution of the benchmark revealed a number of deviations from the model described in chapter \ref{chap:model}. The first becomes apparent when looking at figure \ref{fig:appendix_hd4000_wgsize}, depicting the execution time for an OpenCL computation with a varying number of work items in the work-group. In contrast to the result of the GT-650M (see figure \ref{fig:model_wgsize}), the HD 4000 does not exhibit the inverse exponential behavior discussed in section \ref{sect:model_wgsize}. Instead, whenever work-group size $w \operatorname{mod} 16 = 1$, there was a spike in the run time, with the first spike having the greatest effect on this and later ones becoming gradually smaller. This provides a clue as to the degree of parallelism. In OpenCL each work-group holds a fixed number of work items that is specified prior to the execution of the kernel. For the execution of work items from a new work-group to be initiated, all the work items in the prior work-group have to finish their execution. The Intel HD4000 has 16 execution units. Hence, with a work-group size of 17 the utilization will be $\frac{17}{32}$, with a work-group size of 33 the degree of utilization will be $\frac{33}{48}$, and so on. This can be generalized with the formula shown in equation \ref{eq:future_othergpus_hd4000_wgsize_util}, leading to the multiplicity function seen in equation \ref{eq:future_othergpus_hd4000_wgsize_multiplicity}.

\begin{align}
	\label{eq:future_othergpus_hd4000_wgsize_util} U(w, n_{XU}) &= \frac{w \operatorname{mod} n_{XU}}{n_{XU}} + \frac{\lfloor \frac{w}{n_{XU}}\rfloor}{\lceil \frac{w}{n_{XU}}\rceil} \\
	\label{eq:future_othergpus_hd4000_wgsize_multiplicity} M(t_{max}, U) &= \frac{t_{U_{max}}}{U}
\end{align}

In terms of the kernel itself, basic operations appear to bring no costs at all with them. Measurements (see figure \ref{fig:appendix_hd4000_ops}) reveal the costs of basic operations to be negligible, and the Standard Deviation is judged to be too great to make reliable predictions. This effect could be researched more thoroughly, in order to determine whether the basic operations are shadowed by other operations in some manner. \\

Memory accesses made within the kernel also show a different behavior compared to the one discussed in section \ref{sect:model_access}. Consider measurements taken from the various kinds of memory accesses\footnote{The same kinds of accesses as described in section \ref{sect:model_access_classes}.} shown in figure \ref{fig:appendix_hd4000_access} in the appendix. Here, a number of discrepancies that could be profitably addressed in future revisions of the runtime prediction model, are discussed. Firstly, there is a performance anomaly occurring when handling more than 10 million elements. After that point the results stopped increasing linearly, as expected. Instead, the kernels were observed to be executing \textit{faster}, even though the kernel was executed with a greater number of elements. This effect could be researched further. \\

The fact that there seem to be very little costs attached to constant and interval accesses also hints at caches with fast access speeds. Also, a single continuous access also does not have a major impact on the run time. Significant increases in the run time behavior may only be observed when there is a second continuous access performed within a kernel. This effect occurs both for memory accesses that reference different addresses, as well as for accesses referencing the same addresses. This could also be researched further and incorporated into the model. 

\subsection{AMD Radeon HD 5750}
\label{sect:future_othergpus_amd5750}
The AMD Radeon HD 5750 is mid-range desktop GPU released in 2009. It was among the first AMD GPUs to support OpenCL. It is clocked at 700MHz, with nine execution units, and 1GB GDDR5 video memory. The maximum work-group size is 256. \\  \cite{guru2009hd5750,amd2009hd5750}

The execution of the benchmark suite reveals a behavior similar to the one observed with the NVidia GPUs. Basic operations behave the same way, except that the time they take to complete is not observable for single operations. For multiple operations, the observed behavior is the same as with the GT-650M. Again, the first few operations do not lead to a significant increase in execution time, afterwards, there is a constant amount of time added for each operation. The measurements are depicted in figure \ref{fig:appendix_hd5750_multiple_float}. \\

The runtime behavior in respect to the size of the work-group resembles the one observed in the Intel HD4000. Consider figure \ref{fig:appendix_hd5750_wgsize} for the data gathered experimentally. Again, the runtime decreases as the size of the work-group increases, up to a work-group size of 64. Afterwards, the execution time doubles, and starts decreasing again. \\

When taking measurements on the HD 5750, performance anomalies similar to the one occurring on the HD 4000 were observed. While the measurements that were taken indicate a suitability of the runtime prediction model for AMD GPUs, further measurements could be taken in order to gather valid data for all cost types and therefore verify that claim. 

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work on the compiler}
\label{sect:future_compiler}

The funkyIMP OpenCL backend supports a subset of all the available operations in the funkyIMP programming language, as described in chapter \ref{chap:compiler}. The functionality is enough to perform basic computations, however there are more language features that have to be implemented in order to use it in a productive manner. \\

\paragraph{Full Type System Support} has not been implemented as yet. Currently the compiler backend only supports iterations over multidimensional arrays. This is only a subset of the abilities of the type system. For example, it is possible to describe subsets of such an array. The following example describes a partition of a matrix into three parts, i.e. the trace, the upper and the lower triangle.

\begin{align}
	\operatorname{domain} \operatorname{trace}\{x,y\} : one\_d\{x\}(j) &= \{ two\_d\{x,y\}(a,b)\ |\ j=a \wedge a=b \wedge x=y \} \\
	\operatorname{domain} \operatorname{utriag}\{x,y\} &= \{ two\_d\{x,y\}(a,b)\ |\ b \geq a+1 \wedge x=y \} \\
	\operatorname{domain} \operatorname{ltriag}\{x,y\} &= \{ two\_d\{x,y\}(a,b)\ |\ b<a \wedge x=y \} 
\end{align}

Domain iterations are also supported on these projections. The OpenCL compiler backend, however, does not support them. To implement support for these, the code generation has to be modified slightly. First, the host code needs to copy the whole memory segment that houses the domain. Implementing this feature is an opportunity to optimize the copying of memory between device and host. For example, if an iteration over the upper triangle is followed by an iteration over the lower triangle, the memory segment that contains both of them should only be copied once, as it cannot be changed owing to the linear type constraint. \\

The kernel will be executed with as many work items as there are elements in the domain. In the kernel there needs to be a mapping from the current work item id to the coordinates of the element that is being worked on presently, to access the correct element in the memory segment. Projections may also be stacked, so there might be multiple translations until the actual type is reached. \\

\paragraph{Nested Domain Iterations} are another concept that needs to be supported. Currently, there is no possibility to execute a piece of code more then once, as OpenCL does not allow recursion, and the funkyIMP language only allows loops in the form of domain iterations. The current OpenCL backend does not support this, however. For the code executed on the GPU, the barvinok\footnote{library for counting the number of points in polytopes. (\url{http://freecode.com/projects/barvinok})} library can translate the domain iterations to \code{for}-loops. These loops could then be used in the OpenCL kernel, after making sure the identifiers are unique. For loops that have a defined size, loop unrolling may be applied in order to reduce the overhead caused by the control flow operations. \\ 

Loops may also present a challenge for the Runtime model, as the run time may not scale linearly with the number of iterations in the loop. There might be effects due to the caches used that might change the run time, so one cannot simply multiply the run time for a single run with the number of times the loop body is executed. Furthermore, control flow instructions are expensive, and need to be additionally factored into the run time prediction. \\

\paragraph{Dynamic Arrays} should be straightforward to implement. These cannot change the size of their dimensions within the domain iteration, so they behave just as their static counterparts from the OpenCL point of view. For the host side, the static information from the domain type that is used to determine the number of work items, and the size of the memory allocated on the GPU, needs to be replaced by function calls to the funkyIMP runtime. \\

\paragraph{Object-Oriented concepts} might prove difficult to implement. The FunkyIMP language supports classes, inheritance and polymorphy. It is possible to map object-oriented structures with virtual function calls to C, for example by adding pointers to the addresses of virtual functions as fields of the structure modeling the translated class. However, there is the restriction in OpenCL C that pointers to functions are specifically not allowed. Another approach might be to generate OpenCL C code for all possible matches of the object, and then choosing the appropriate implementations, and optionally the superclass implementation, dynamically at runtime using a field indicating the runtime type of the object. Unfortunately, this approach does not work for code that is not known at compilation time, which makes its use in libraries difficult.\\